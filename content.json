{"pages":[{"title":"About","text":"目前就读于武汉大学，即将当一名出国狗。 博客用来记录生活和技术，欢迎大家一起交流。 有问题请联系我：shuoc1218@163.com","link":"/about/index.html"},{"title":"浅谈机器学习与深度学习中的算法原理(一)","text":"当今社会，各行各业无不涉及人工智能和机器学习，掌握其中的基本原理知道如何应用至关重要，同时也是求职者必备技能。本系列博客旨在介绍机器算法和深度学习的基本原理，为进行更深层次的应用打下基础。 本篇文章主要介绍传统学习和机器学习的内容和主要区别。 什么是机器学习？机器学习的定义“A computer program is said to learn from experience E with respect tosome class of tasks T and performance measure P if its performance attasks in T, as measured by P, improves with experience E ” 翻译过来是“一个电脑程序要完成任务(T),如果电脑获取的关于T的经验(E)越多就表现得(P)越好,那么我们就可以说这个程序学习了”。 更直白一点：“经验越多，学习越好”。 比如：根据身高预测体重，根据气压、湿度、风力等预测降雨概率等。 这里需要注意的是，这些机器学习的特征，是人工提前设置好的，在进行这项工作之前，是由人类专家根据自己的以往经验来选取的重要的特征，然后机器通过分析这些特征的历史数据，来找到相应的模式，即不同特征组合下的不同结果。 什么是深度学习？深度学习的定义 “Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of lessabstract ones.” 翻译过来是“深度学习是一种特殊的机器学习，它可以获得高性能也十分灵活。它可以用概念组成的网状层级结构来表示这个世界，每一个概念更简单的概念相连，抽象的概念通过没那么抽象的概念计算。” 以人类判断方形和圆形为例，在判断过程中，我们首先看这个形状有没有四条边，有的话进一步判断是否垂直，是否等长。如果以上条件都能满足，则是一个这正方形。在这个过程中，我们的大脑自动将识别的整个复杂的任务分为了几个简单的小块。 传统机器学习与深度学习有何区别？在识别猫和狗的过程中，若使用传统机器学习的算法，我们需要定义一些特征，告诉机器从哪些方面进行判断，比如，胡须、耳朵、鼻子、嘴巴的模样。首先要确定相应的面部特征，作为机器学习的特征，以便分类判断。 而深度学习的工作流程如下：1. 首先确定出有哪些边和角跟识别出猫狗关系最大; 2.根据上一步找出的很多小元素（边、角等）构建层级网络，找出它们之间的各种组合; 3.在构建层级网络之后，就可以确定哪些组合可以识别出猫和狗。 深度学习方法的优越性就体现在，它能自动找到这个问题所需要的重要特征，而传统机器学习需要手工的给出特征。 以人脸识别为例：可以看到4层，输入的是Raw Data，就是原始数据，这个机器没法理解。于是，深度学习首先尽可能找到与这个头像相关的各种边，这些边就是底层的特征（Low-level features），这就是上面写的第一步；然后下一步，对这些底层特征进行组合，就可以看到有鼻子、眼睛、耳朵等等，它们就是中间层特征（Mid-level features），这就是上面写的第二步；最后，我们队鼻子眼睛耳朵等进行组合，就可以组成各种各样的头像了，也就是高层特征（High-level features）这个时候就可以识别出或者分类出各种人的头像了。 对比机器学习和深度学习，可以发现以下不同点： 数据依赖 随着数据量增加，两者表现有很大区别： 数据集较大时，可以使用深度学习方法；数据集较小，用传统机器学习方法或许更合适。 硬件依赖 深度学习十分依赖于高端的硬件设施，因为计算量巨大。深度学习涉及很多矩阵运算，因此很多深度学习都涉及GPU运算，对比之下，机器学习要求配置较低。 特征工程 在机器学习中，几乎所有特征都业内专家确定，手工对特征进行编码量化。 深度学习中，算法自动从数据中学习特征，大大降低发现特征的成本。 解决问题的方式 传统机器学习算法需要将问题分成几块，一个个解决好之后重新组合起来。 深度学习端到端，一次性解决好。 在物体检测方面，机器学习做法分两步，发现物体（找到物体的位置），识别物体（确定是什么类型的物体）。而在深度学习中，给它一张图，它能直接把物体识别出来，同时标明物体的名字，如yolo算法。 运行时间 深度学习需要花大量时间来训练，ResNet需要两周时间训练。机器学习几秒钟到几个小时就可以训练好。但是机器学习模型一旦训练好，在预测任务上面就会表现很快，如上图视频中实时识别。 可理解性 深度学习就像一个黑盒子，每一层代表一个特征，里面的层多了，它所表示的特征就很难以理解。我们没法对训练出来的模型对于预测任务做一个合理的解释。机器学习则不存在这一问题，在决策树算法中，我们能将每一个规则都能列出来。 参考机器学习和深度学习的科普文：https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/ 常见的机器学习算法理解传统机器学习从一些观测（训练）样本出发，试图发现不能通过原理分析获得的规律，实现对未来数据行为或趋势的准确预测。 相关算法包括： K 近邻方法 决策树方法 三层人工神经网络方法 Adaboost算法 贝叶斯方法 逻辑回归 隐马尔科夫方法 支持向量机方法","link":"/draft/浅谈机器学习与深度学习中的算法原理-一.html"},{"title":"","text":"tensorflow基本概念 使用图(graph)来表示计算任务。 在被称之为会话(Session)的上下文(context)中执行图。 使用tensor表示数据 通过变量(Variable)维护状态 使用feed和fetch可以为任意的操作赋值或者从其中获取数据。 Tensorflow是一个编程系统，使用图表示计算任务，图中的节点称之为op,一个op获得0个或多个Tensorn，执行计算，产生0个或多个Tensor。Tensor看作是一个n维的数组或列表。图必须在会话里被启动。 softmax函数常用于分类，转换为每一类的概率，总概率和为1。 二次代价函数 用梯度下降的话，参数调整不是很合理 交叉熵函数：换一个思路，不改变激活函数，改变代价函数用交叉熵函数。在交叉熵函数中，权值和偏执值调整与激活函数倒数无关。预测值减实际值（误差）大时，梯度越大，调整越快。 如果输出神经元是线性的，二次代价函数是一种合适的选择。输出神经元是S型函数，比较适合用交叉熵。 对数释然代价函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可用交叉熵。深度学习更普遍将softmax作为最后一层，此时代价函数常用对数释然函数。 对数似然函数与softmax组合 和 交叉熵与sigmoid函数组合非常相似。 防止过拟合： 增加数据集 正则化方法 是比较小的权值接近于0，这个神经元不存在，减小网络复杂程度 Dropout 训练过程中每一次迭代过程中使某些神经元不工作，测试时激活所有神经元。 优化器： 标准梯度下降法 随机梯度下降法 批量梯度下降法 SGD Momentum:当前权值改变受到上一次权值改变的影响。 NAG：提前知道方向。 Adagrad：对于比较常见的数据给予比较小的学习率调整参数，罕见的用较大的学习率调参，适合于数据稀疏的数据集。优势在于不需要人为调节学习率，自动调节，缺点是随着迭代次数变多，学习率也会越来越低，最终趋向于0。 RMSprop: RMS是均方根缩写。不会出现学习率越来越低的问题，也会自己调节学习率。 Adadelta: 不需要设置学习率。 Adam: xxxx 从准确率上来讲，随机梯度下降还是比较可靠的。 Adadelta，Adam 模型收敛速度比较快。 在创建新的网络时候，调试网络时候可以使用速度比较快的网络(Adadelta，Adam)，最后发论文出结果的时候把所有优化器都试一遍，看谁结果准确率最高。 卷积神经网络：通过感受野和权值共享减少了神经网络需要训练参数的个数。 RNN（Recurrent Neural Network): 处理语音或是文字时不能单独处理，需要看成一个连续整体，将上一个输出作为下一个输入，帮助决策。 会有梯度消失问题，若换成y=x，但是一直不会减弱，会记录所有结果，重要的不重要的全会记住。我们要有选择性的记忆。 LSTM (Long Short Term Memory) block放在隐藏层的位置，有三个门：输入门，忘记门，输出门 使用已有模型进行训练： 卷积层是为了做特征的提取，这部分对于分类自己的数据是比较适用的。 交替训练：不同任务，不同训练集（英语-&gt;法语/德语) 联合训练：，不同任务，相同训练集。（多个数字的验证码识别） 截图：http://www.cnblogs.com/handy1998/p/9982646.html","link":"/draft/tensorflow.html"},{"title":"浅谈机器学习算法三","text":"","link":"/draft/浅谈机器学习算法三.html"},{"title":"","text":"title: 浅谈机器学习与深度学习中的算法原理(二)date: 2018-12-15 09:41:12tags: 本文意在讨论机器学习算法中的K-近邻算法和决策树算法。 一.K-近邻算法1.概述K-近邻算法，又称为KNN算法(K-Nearest Neighbor)，是数据挖掘中最简单的算法。 工作原理为：给点给一个已知标签类别的训练集，输入没有标签的新数据集后，在训练数据中找到与新训练集最邻近的K个实例。如果这K个实例的多数属于某个类别，那么新数据就属于这个类别。简单理解为：由那些离X最近K个点投票来决定将X归为哪一类。 如上图所示，绿色圆点的分类情况和我们选取的K的值有关。 当K=3时，即我们选择和绿点距离最近的三个点，此时绿点被划分到红色三角形这一类。 当K=5时，即我们选择和绿点距离最近的五个点，此时绿点被划分到蓝色正方形这一类。 2.实现用python3代码实现上述过程，先计算距离，再将距离排序，统计前K个点所出现的频率，灾后确定类别。 先空着，留坑///// 二. 决策树算法1.概述决策树(Decision Tree)是有监督学习的一种算法，同时是一种基本的分类和回归算法。分为两种：分类树和回归树。 以决定是否出门为例，我们可以画出下面的回归树。 可以看出，决策树本质是树形结构，我们通过设计一些条件来对数据进行分类。 决策树由节点和有向边组成，一般一颗决策树包含一个根节点、若干内部节点和若干叶节点。决策树的决策过程需要从决策树的根节点出发，将待测数据与决策树中的特征节点进行比较，并按照比较结果选择下一个分支，直到到达叶子节点作为最终的决策结果。 2.决策树构建 特征选择 特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树的学习效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大的差别，则称这个特征是没有分类能力的。通常特征选择的准则是信息增益或信息增益比。 在决策树的构建中，可以通过选择不同的特征作为根节点，这样形成的决策树也都能延续下去。问题是：怎样选择更好？这就要求我们确定选择的原则，直观上来看：如果一个特征具有更好的分类能力，或者说，根据这个特征将训练集分割为不同的子集，使得各个子集在当前条件下有最好的分类，就应该选择这样的特征。信息增益（information gain）就能很好的表达这一直观准则。 在了解信息增益之前，有必要给出熵和信息熵的定义。 在信息论与概率论统计中，熵 (entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，概率分布为$$P(X=X_i)=p_i, i=1,2,3…$$则随机变量X的熵定义为$$H(X) = -\\sum_{i=1}^{N}p_ilogp_i$$ 在上式定义中，$p_i=0$，则定义0log0=0。通过，式中对数以2为底或以自然对数e为底，熵的单位分别称作为比特(bite)或纳特(nat)。由定义可知，熵只依赖于X的分布，而与X的取值无关，所以可将X的熵记作H(p)。 熵越大，随机变量的不确定性就越大，从定义可以验证$$0\\leq H(p)\\leq logn$$当随机变量只取0,1时，X的分布为$$P(X=1)=p,P(X=0)=1-p,0\\leq p \\leq 1$$熵为$$H(p)=-plog_2p-(1-p)log_2(1-p)$$作出H(p)图像如图所示，单位为比特。 当p=0或p=1时H(p)=0， 随机变量完全没有不确定性。当p=0.5时，H(p)=1，熵取值最大，随机变量的不确定性最大。 条件熵H(Y|X)表示在一直随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望。$$H(Y|X)=\\sum_{i=1}^{n}p_iH(X=x_i)$$这里，$p_i=P(X=x_i),i=1,2,…,n$ 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少程度。 信息增益 特征A对训练数据集D的信息增益为g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$$g(D,A)=H(D)-H(D|A)$$一般的，熵H(Y)与条件熵H(Y|X)之差成为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 显然的，信息增益表示特征A对数据集D分类的不确定性减少程度，不同的特征具有不同的信息增益。信息增益大的特征具有更强的分类能力。 设训练数据为D，|D|表示其样本容量，即样本个数。设有K的类C_k,k=1,2,\\cdots ,K,|C_k|为属于类C_k的样本个数，$\\sum_{k=1}^K|C_k|=|D|$即特征有个不同的特征.即特征A有n个不同的特征{a_1,a_2,a_3,\\cdots, a_n}根据特征的取值将划分为个子集,根据特征A的取值将D划分为n个子集D_1,D_2,D_3,\\cdots ,D_n,|D_t|为为D_i的样本个数，的样本个数，\\sum_{i=1}^n=|D|。记子集。记子集D_i中属于类中属于类C_k的样本集合为的样本集合为D_{ik}，即，即D_{ik}=D_i\\bigcap C_k,|D_{ik}|为为D_{ik}$的样本个数，信息增益的算法如下。 输入：训练数据集D和特征A; 输出:特征A对训练数据集D的信息增益g(D,A). (1)计算数据集D的经验熵H(D)$$H(D)=-\\sum_{k=1}^K \\frac{C_k}{|D|}log_2\\frac{|C_k|}{D}$$(2)计算特征A对数据集D的经验条件熵H(D|A)$$H(D|A)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^{n}\\frac{|D_{i}|}{|D|}\\sum_{i=1}^{n}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_{i}|}$$PS:在特征的每个子集中不是简单求和，而是加权求和 (3) 计算信息增益$$g(D,A)=H(D)-H(D|A)$$ 信息增益比 信息增益值的大小是相对于训练数据集而言，没有绝对的意义。在分类问题困难时，也就是训练数据集的经验熵大的时候，增益值也会偏大。所以使用信息增益比(information gain ratio)可以对这一问题进行校正。 特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：$$g_R(D,A)=\\frac{g(D,A)}{H)(D)}$$ 3.决策树的生成 ID3 生成算法 方法原理:从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树;知道所有特征的信息增益均很小或没有特征可以选择为止。最后得到决策树。ID3相当于用极大似然法进行概率模型的选择。 算法： (1)若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该节点的类标记，返回T; (2)若A=∅，则T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T; (3)否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$; (4)如果$A_g$的信息增益小于阈值$\\epsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回T; (5)否则，$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$,将$D_i$中的实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T; (6)对第i个子节点，以$D_i$为训练集，以A-{$A_g$}为特征集，递归地调用步(1)~步(5)，得到子树$T_i$,返回$T_i$。 ID3算法只有树的生成，所以该算法内容生成的树容易出现过拟合。 C4.5的生成算法 C4.5算法与ID3算法相似，进行了一些改进，在生成树的过程中，用信息增益比来选择特征。 4.决策树的剪枝…天坑","link":"/draft/浅谈机器学习与深度学习中的算法原理-二.html"}],"posts":[{"title":"Say goodbye to 2018","text":"2018年还有20多天结束了，刚好在这天迎来了自己20岁的生日，人生这么快就过去了六分之一。每思致此，我还有些惆怅。 正好最近在研究 hexo+github 搭建自己的个人博客，就以此作为博客的第一篇。 总的来评价，我的2018年过的还是比较有意义的。年头我就想要在这一年里面把之前没有时间去完成的事情多完成几件，尽量去尝试多种不同的有意义的事情。 上半年主要是上专业课+学习爬虫，虽然爬虫知识到现在为止漏洞还是很多。这门兴趣我是在大学生科研阶段培养起来的，偶然网上看到静谧大神，跟着他的博客一步步学习下去，有一点小小的成果和经验，但是尚缺的还是从一个系统来整体学习。在这段时间里面，我记得当时准备考研的同学已经开始着手准备了，那时的我也是准备考研的，但是我一点不慌，每天也是跟着他们去图书馆，看博客、读爬虫书、亲自实验，玩的有滋有味。 三月份的时候接触到了一群玩街头健身的朋友，每天基本无特殊情况的话都会在信操一起锻炼。接触的途径是有一天晚上闲的无聊去东湖散步，突然想到咱们学校有没有那种健身社团，于是打开qq群在附近搜索“武大健身”关键字，然后就加入了组织。在这个圈子中，大家三观都很正，而且都是那种积极上进的人，无论在健身还是学习方面，所以自己还是比较喜欢这里的。 一直到暑假前夕，我才开始关注一下未来的出路了，当时自己还在张同学的CS鼓吹论下考虑要不要转浙大计算机，也了解了一下北航、复旦、浙大、中科大这些学校的计算机专业，都蛮强的 (这是废话)，甚至还一时冲动还在某二手平台买了408专业课的复习资料准备开始复习，后来由于个人懒惰原因没能坚持读下去。 为了调适心情（玩+赚钱），我参加了某研学公司去参加带小孩子夏令营兼职了，5天湖南游，大夏天舍命陪孩子玩儿，赚了几百块钱，那时候头哥和zy也在湖南的公司实习，但是由于我这边比较忙连见面时间都没有。完成兼职之后还是没开始复习，又头铁去参加了感知成都的暑期实践，成都毕竟一直以来也是我想去的地方，有美食，美女，美景还有萌萌的大熊猫，一个礼拜的成都之行还是蛮巴适的。 再后来已经到了7月10几号了，开始紧张起来，一开始准备在学校复习考研的，但是每天感觉睡眠不好，也可能由于夏天到了心思静不下来，想回去学习了。回去之后果不其然，睡眠挺好的，每天睡觉睡的起不来，懒惰的要死，低效率的学了两个礼拜之后又在7月底早早的来到了学校。当时香蕉哥正一起复习，那段时间是整个一学年的高效时期，每天生活很规律，学的很有劲，而且每天还能抽出时间一起锻炼。 开学后，一个转折点出现了。那段时间我们大学生科研要结题了，就去找桂老师聊了一聊了。那天聊结题的内容什么的我忘了，但是记得的是就是这次聊天把我从考研拉回了出国(哎，真是一个善变的男人)。当时花了几天考虑了一下，毕竟之前复习的还是卓有成效的，记得是在欢乐谷的S1000过山车上考虑清楚的。 从那以后我就开始准备各种材料，看学校信息，准备德国的留学审核部审核(好像到现在为止还没开始复习)。每天写材料写的痛苦不堪，拿给czy改的时候又被diss好久，导致我很失落那段时间。但是还好，虽然英文写作是比较菜，但是慢慢磨，总会有个材料出来。拿着这份材料交了几个学校，录了一个港理工(我记得还把academic后面多加了一个e)。本来准备拿这个学校保底的，但是这货居然让我12月份交留位费，上了张鬼子的当，然后又提交了几个学校，这些还在等结果，希望这个月能再收到几个offer稳稳军心吧，当然自己的梦校还是TUM。 其实每条路都有存在的意义，坚持走完都有成果的。无论未来碰到什么石头，都一定要自信的走下去，不动摇。Follow your heart and achieve your goals. 2018年12月6日 于 武汉大学信息学部图书馆","link":"/2018/12/06/Say good bye to 2018/"},{"title":"labelme","text":"labelme的安装和使用因为最近是本科毕业设计开题，我的毕设题目是融合遥感影像和GPS轨迹数据的道路提取，涉及到深度学习来提取影像信息，数据源需要用labelme来进行标注。 labelme是MIT开源的一款标注软件，官方的github地址为https://github.com/wkentaro/labelme. 一. 安装过程本文介绍labelme在windows下的安装过程。 在Windows系统下，首先安装anaconda, 官网下载 https://www.anaconda.com/ ，博主这里下载的是3.7版本，安装过程中记得把添加到系统路径勾上，否则就安装完成之后手动添加。 安装完成之后，开始菜单打开anaconda prompt, 输入以下命令。 12345# python3conda create --name=labelme python=3.7activate labelmeconda install pyqt # pyqt5 can be installed via pip on python3pip install labelme # use &apos;pip install&apos; instead of &apos;conda install&apos; 安装至此结束，是不是很简单呢。 二.使用方法Labelme可对图像进行矩形标注和多边形标注，生成的文件是json文件。 在anaconda prompt 窗口下继续输入命令： labelme 即可打开labelme的图形界面。 仔细观察的话，可以发现左上角是一个Lena.jpg哦。","link":"/2018/12/11/labelme/"},{"title":"deep learning ai notes","text":"Coursera Enda Wu: Courses in this sequence(Specialization): Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization. Structuring your Machine Learning project. Convolutional Neural Networks. Natural Language Processing: Building Sequence Models 1.Neural Networks and Deep Learningweek 1Relu(rectified linear unit) $max(0,w^Tx+b)$ Different types of neural network: CNN(Convolutional Neural Network) used often for image application and RNN(Recurrent Neural Network) used often for one-dimension sequence data such as text transcript. Hybrid neural network architecture for autonomous driving. Structured and Unstructured data Structured data refers to things that has a defined meaning, such as price and age. Unstructured data refers to things like pixel, raw audio, text. outline of this course week 1: Introduction week 2: Basic of Neural Network programming week 3: One hidden layer Neural Networks week 4: Deep Neural Networks week 2 logistic regression","link":"/2018/12/11/courseradl/"},{"title":"Markdown的基本用法介绍","text":"markdown语言是一种纯文本格式语言，通过简单的标记语法，它可以使普通文本具有一定的格式。本博客使用的就是该种语言。为此，我们有必要熟悉一下这种语言，下面我们花5分钟时间来简单了解一下吧。 一. 标题12345# 一级标题## 二级标题### 三级标题...###### 最多能支持六级标题 这里需要注意的是，#后面要跟个空格再写文字。 二. 字体 加粗 在要加粗的文字左右分别用两个**包起来。 或按快捷键Ctrl+B 斜体 在要倾斜的文字左右分别用一个*包起来。 或按快捷键Ctrl+I 斜体加粗 加起来就是左右三个* 啦 或快捷键两个先后使用 删除线 删除内容左右分别用两个~~包起来。 或快捷键Alt+Shift+5 三. 引用在引用的文字前加&gt;即可。引用可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt;，不限制数量。 这是引用内容 这也是引用内容 四.分割线 使用三个或三个以上的-或是*。 效果完全相同 五. 图片1234![alt](图片地址 &apos;&apos;title&apos;&apos;)![blockchain](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=702257389,1274025419&amp;fm=27&amp;gp=0.jpg &quot;区块链&quot;)alt是图片下面显示的标题，title是鼠标移动到图片上显示的文字，title可省略 六. 超链接12[超链接名](超链接地址 &quot;超链接title&quot;)title可省略 百度 七.列表 无序列表 用 -、+、*任何一种效果相同，注意后面跟空格再加文字。 1- 测试1 测试2 有序列表 用数字加.，序号和内容之间有空格。 列表嵌套 上一级和下一级之间敲三个空格 一级无序 二级无序 三级无序 一级有序​ 二级无序​ 二级无序 八. 表格123456789101112131415表头|表头|表头-|:-:|-:内容|内容|内容内容|内容|内容第二行分割表头和内容文字默认居左，两边加：居中，右边加：居右注：原生的语法两边都要用|包起来实例：姓名|技能|排行--|:--:|--:刘备|哭|大哥关羽|打|二哥张飞|骂|三弟 姓名 技能 排行 刘备 哭 大哥 关羽 打 二哥 张飞 骂 三弟 九.代码单行代码：代码之间分别用一个反引号`包起来 create database heros 代码块：用三个反引号`包起来,且引号要单独占一行 1234#include&lt;stdio.h&gt;void main{ printf(&quot;hello world&quot;);} 十. 流程图123456789​```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; 123456789101112```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; ` 后续再编辑过程中若还有新用法持续更新…","link":"/2018/12/08/markdown基本用法/"}],"tags":[{"name":"年度总结","slug":"年度总结","link":"/tags/年度总结/"},{"name":"数据标注","slug":"数据标注","link":"/tags/数据标注/"},{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"博客搭建","slug":"博客搭建","link":"/tags/博客搭建/"}],"categories":[{"name":"生活随笔","slug":"生活随笔","link":"/categories/生活随笔/"},{"name":"技术杂谈","slug":"技术杂谈","link":"/categories/技术杂谈/"},{"name":"课程笔记","slug":"课程笔记","link":"/categories/课程笔记/"}]}